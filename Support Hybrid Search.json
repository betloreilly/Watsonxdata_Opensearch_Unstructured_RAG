{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-UTv2m",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "LanguageModelComponent-dNecf",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-Prompt-UTv2m{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-UTv2mÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-LanguageModelComponent-dNecf{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“LanguageModelComponent-dNecfÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "selected": false,
        "source": "Prompt-UTv2m",
        "sourceHandle": "{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-UTv2mÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "LanguageModelComponent-dNecf",
        "targetHandle": "{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“LanguageModelComponent-dNecfÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-DZupy",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "search_query",
            "id": "OpenSearch-M8Kna",
            "inputTypes": [
              "Message"
            ],
            "type": "query"
          }
        },
        "id": "reactflow__edge-ChatInput-DZupy{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-DZupyÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-OpenSearch-M8Kna{Å“fieldNameÅ“:Å“search_queryÅ“,Å“idÅ“:Å“OpenSearch-M8KnaÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“queryÅ“}",
        "selected": false,
        "source": "ChatInput-DZupy",
        "sourceHandle": "{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-DZupyÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "OpenSearch-M8Kna",
        "targetHandle": "{Å“fieldNameÅ“:Å“search_queryÅ“,Å“idÅ“:Å“OpenSearch-M8KnaÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“queryÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "OpenAIEmbeddings",
            "id": "OpenAIEmbeddings-74cee",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          },
          "targetHandle": {
            "fieldName": "embedding",
            "id": "OpenSearch-M8Kna",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-OpenAIEmbeddings-74cee{Å“dataTypeÅ“:Å“OpenAIEmbeddingsÅ“,Å“idÅ“:Å“OpenAIEmbeddings-74ceeÅ“,Å“nameÅ“:Å“embeddingsÅ“,Å“output_typesÅ“:[Å“EmbeddingsÅ“]}-OpenSearch-M8Kna{Å“fieldNameÅ“:Å“embeddingÅ“,Å“idÅ“:Å“OpenSearch-M8KnaÅ“,Å“inputTypesÅ“:[Å“EmbeddingsÅ“],Å“typeÅ“:Å“otherÅ“}",
        "selected": false,
        "source": "OpenAIEmbeddings-74cee",
        "sourceHandle": "{Å“dataTypeÅ“:Å“OpenAIEmbeddingsÅ“,Å“idÅ“:Å“OpenAIEmbeddings-74ceeÅ“,Å“nameÅ“:Å“embeddingsÅ“,Å“output_typesÅ“:[Å“EmbeddingsÅ“]}",
        "target": "OpenSearch-M8Kna",
        "targetHandle": "{Å“fieldNameÅ“:Å“embeddingÅ“,Å“idÅ“:Å“OpenSearch-M8KnaÅ“,Å“inputTypesÅ“:[Å“EmbeddingsÅ“],Å“typeÅ“:Å“otherÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "OpenSearch",
            "id": "OpenSearch-M8Kna",
            "name": "search_results",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_data",
            "id": "parser-DpXp4",
            "inputTypes": [
              "DataFrame",
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-OpenSearch-M8Kna{Å“dataTypeÅ“:Å“OpenSearchÅ“,Å“idÅ“:Å“OpenSearch-M8KnaÅ“,Å“nameÅ“:Å“search_resultsÅ“,Å“output_typesÅ“:[Å“DataÅ“]}-parser-DpXp4{Å“fieldNameÅ“:Å“input_dataÅ“,Å“idÅ“:Å“parser-DpXp4Å“,Å“inputTypesÅ“:[Å“DataFrameÅ“,Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "selected": false,
        "source": "OpenSearch-M8Kna",
        "sourceHandle": "{Å“dataTypeÅ“:Å“OpenSearchÅ“,Å“idÅ“:Å“OpenSearch-M8KnaÅ“,Å“nameÅ“:Å“search_resultsÅ“,Å“output_typesÅ“:[Å“DataÅ“]}",
        "target": "parser-DpXp4",
        "targetHandle": "{Å“fieldNameÅ“:Å“input_dataÅ“,Å“idÅ“:Å“parser-DpXp4Å“,Å“inputTypesÅ“:[Å“DataFrameÅ“,Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LanguageModelComponent",
            "id": "LanguageModelComponent-JQa1y",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "hybrid_search_query",
            "id": "OpenSearch-M8Kna",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-LanguageModelComponent-JQa1y{Å“dataTypeÅ“:Å“LanguageModelComponentÅ“,Å“idÅ“:Å“LanguageModelComponent-JQa1yÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-OpenSearch-M8Kna{Å“fieldNameÅ“:Å“hybrid_search_queryÅ“,Å“idÅ“:Å“OpenSearch-M8KnaÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "selected": false,
        "source": "LanguageModelComponent-JQa1y",
        "sourceHandle": "{Å“dataTypeÅ“:Å“LanguageModelComponentÅ“,Å“idÅ“:Å“LanguageModelComponent-JQa1yÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "OpenSearch-M8Kna",
        "targetHandle": "{Å“fieldNameÅ“:Å“hybrid_search_queryÅ“,Å“idÅ“:Å“OpenSearch-M8KnaÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-jLz8p",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "LanguageModelComponent-JQa1y",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-Prompt-jLz8p{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-jLz8pÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-LanguageModelComponent-JQa1y{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“LanguageModelComponent-JQa1yÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "selected": false,
        "source": "Prompt-jLz8p",
        "sourceHandle": "{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-jLz8pÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "LanguageModelComponent-JQa1y",
        "targetHandle": "{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“LanguageModelComponent-JQa1yÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-DZupy",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "question",
            "id": "Prompt-jLz8p",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-ChatInput-DZupy{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-DZupyÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-Prompt-jLz8p{Å“fieldNameÅ“:Å“questionÅ“,Å“idÅ“:Å“Prompt-jLz8pÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "selected": false,
        "source": "ChatInput-DZupy",
        "sourceHandle": "{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-DZupyÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "Prompt-jLz8p",
        "targetHandle": "{Å“fieldNameÅ“:Å“questionÅ“,Å“idÅ“:Å“Prompt-jLz8pÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "parser",
            "id": "parser-DpXp4",
            "name": "parsed_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "context",
            "id": "Prompt-UTv2m",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-parser-DpXp4{Å“dataTypeÅ“:Å“parserÅ“,Å“idÅ“:Å“parser-DpXp4Å“,Å“nameÅ“:Å“parsed_textÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-Prompt-UTv2m{Å“fieldNameÅ“:Å“contextÅ“,Å“idÅ“:Å“Prompt-UTv2mÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "selected": false,
        "source": "parser-DpXp4",
        "sourceHandle": "{Å“dataTypeÅ“:Å“parserÅ“,Å“idÅ“:Å“parser-DpXp4Å“,Å“nameÅ“:Å“parsed_textÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "Prompt-UTv2m",
        "targetHandle": "{Å“fieldNameÅ“:Å“contextÅ“,Å“idÅ“:Å“Prompt-UTv2mÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-DZupy",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "question",
            "id": "Prompt-UTv2m",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-ChatInput-DZupy{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-DZupyÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-Prompt-UTv2m{Å“fieldNameÅ“:Å“questionÅ“,Å“idÅ“:Å“Prompt-UTv2mÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "selected": false,
        "source": "ChatInput-DZupy",
        "sourceHandle": "{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-DZupyÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "Prompt-UTv2m",
        "targetHandle": "{Å“fieldNameÅ“:Å“questionÅ“,Å“idÅ“:Å“Prompt-UTv2mÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LanguageModelComponent",
            "id": "LanguageModelComponent-dNecf",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-B13zu",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-LanguageModelComponent-dNecf{Å“dataTypeÅ“:Å“LanguageModelComponentÅ“,Å“idÅ“:Å“LanguageModelComponent-dNecfÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-ChatOutput-B13zu{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“ChatOutput-B13zuÅ“,Å“inputTypesÅ“:[Å“DataÅ“,Å“DataFrameÅ“,Å“MessageÅ“],Å“typeÅ“:Å“otherÅ“}",
        "selected": false,
        "source": "LanguageModelComponent-dNecf",
        "sourceHandle": "{Å“dataTypeÅ“:Å“LanguageModelComponentÅ“,Å“idÅ“:Å“LanguageModelComponent-dNecfÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "ChatOutput-B13zu",
        "targetHandle": "{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“ChatOutput-B13zuÅ“,Å“inputTypesÅ“:[Å“DataÅ“,Å“DataFrameÅ“,Å“MessageÅ“],Å“typeÅ“:Å“otherÅ“}"
      }
    ],
    "nodes": [
      {
        "data": {
          "description": "Get chat inputs from the Playground.",
          "display_name": "Chat Input",
          "id": "ChatInput-DZupy",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "https://docs.langflow.org/chat-input-and-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "context_id",
              "files"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.7.3",
            "metadata": {
              "code_hash": "7a26c54d89ed",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "lfx",
                    "version": null
                  }
                ],
                "total_dependencies": 1
              },
              "module": "custom_components.chat_input"
            },
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chat Message",
                "group_outputs": false,
                "loop_types": null,
                "method": "message_response",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from lfx.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom lfx.base.io.chat import ChatComponent\nfrom lfx.inputs.inputs import BoolInput\nfrom lfx.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom lfx.schema.message import Message\nfrom lfx.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    documentation: str = \"https://docs.langflow.org/chat-input-and-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"context_id\",\n            display_name=\"Context ID\",\n            info=\"The context ID of the chat. Adds an extra layer to the local memory.\",\n            value=\"\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Chat Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        # Ensure files is a list and filter out empty/None values\n        files = self.files if self.files else []\n        if files and not isinstance(files, list):\n            files = [files]\n        # Filter out None/empty values\n        files = [f for f in files if f is not None and f != \"\"]\n\n        session_id = self.session_id or self.graph.session_id or \"\"\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=session_id,\n            context_id=self.context_id,\n            files=files,\n        )\n        if session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"
              },
              "context_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Context ID",
                "dynamic": false,
                "info": "The context ID of the chat. Adds an extra layer to the local memory.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "context_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "files": {
                "_input_type": "FileInput",
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "csv",
                  "json",
                  "pdf",
                  "txt",
                  "md",
                  "mdx",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "list_add_label": "Add More",
                "name": "files",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "temp_file": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Input Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "What are the terms for the product bhv-12132?"
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "selected_output": "message",
          "type": "ChatInput"
        },
        "dragging": false,
        "height": 234,
        "id": "ChatInput-DZupy",
        "measured": {
          "height": 234,
          "width": 320
        },
        "position": {
          "x": 345.0346034220197,
          "y": 614.2329247117536
        },
        "positionAbsolute": {
          "x": 743.9745420290319,
          "y": 463.6977510207854
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-UTv2m",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": [
                "context",
                "question"
              ]
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "error": null,
            "field_order": [
              "template"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "braces",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "legacy": false,
            "lf_version": "1.6.9",
            "metadata": {
              "code_hash": "3bf0b511e227",
              "module": "langflow.components.prompts.prompt.PromptComponent"
            },
            "minimized": false,
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt",
                "group_outputs": false,
                "hidden": null,
                "method": "build_prompt",
                "name": "prompt",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": null,
            "replacement": null,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "context": {
                "advanced": false,
                "display_name": "context",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "context",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "question": {
                "advanced": false,
                "display_name": "question",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "question",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "You are a helpful LEXORA support assistant. Answer the user's question using ONLY the information provided in the context below from the support knowledge base.\n\nINSTRUCTIONS:\n1. Read ALL the context documents carefully\n2. Find the specific section that answers the question\n3. Provide exact numbers, rates, and details from the context\n4. If asked about policies, programs, or procedures, include specific details and any conditions\n5. If the answer is not in the context, say so clearly - do not make up information\n\nCONTEXT:\n{context}\n\nQUESTION: {question}\n\nFORMATTING RULES:\n- When comparing multiple programs, products, or options, use a MARKDOWN TABLE\n- Use tables for: program comparisons, policy comparisons, eligibility matrices, feature lists\n- Use bullet points for: step-by-step processes, requirements, benefits, features, lists\n- Bold important numbers, dates, and key terms\n- Keep responses concise but complete\n- If multiple FAQs are relevant, combine them into a comprehensive answer\n\nTABLE FORMAT EXAMPLE:\n| Program | Eligibility | Coverage | Cost |\n|---------|-------------|----------|------|\n| Care+ Basic | All products | 1 year | $X |\n| Care+ Premium | Electronics only | 2 years | $Y |\n\nANSWER (use tables for comparisons, bullet points for features/processes):\n"
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "prompt",
          "type": "Prompt"
        },
        "dragging": false,
        "height": 433,
        "id": "Prompt-UTv2m",
        "measured": {
          "height": 433,
          "width": 320
        },
        "position": {
          "x": 1937.0914793562094,
          "y": 599.7473229062947
        },
        "positionAbsolute": {
          "x": 1977.9097981422992,
          "y": 640.5656416923846
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "id": "note-iTKDJ",
          "node": {
            "description": "## ðŸ• 2. Retriever Flow\n\nThis flow answers your questions with contextual data retrieved from your vector database.\n\nOpen the **Playground** and ask, \n\n```\nWhat is this document about?\n```\n",
            "display_name": "",
            "documentation": "",
            "template": {
              "backgroundColor": "neutral"
            }
          },
          "type": "note"
        },
        "dragging": false,
        "height": 324,
        "id": "note-iTKDJ",
        "measured": {
          "height": 324,
          "width": 324
        },
        "position": {
          "x": -57.34390299825412,
          "y": 481.47113471301924
        },
        "positionAbsolute": {
          "x": 374.388314931542,
          "y": 486.18094072679895
        },
        "resizing": false,
        "selected": false,
        "style": {
          "height": 324,
          "width": 324
        },
        "type": "noteNode",
        "width": 324
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-B13zu",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "https://docs.langflow.org/chat-input-and-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "context_id",
              "data_template",
              "clean_data"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "metadata": {
              "code_hash": "8c87e536cca4",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "orjson",
                    "version": null
                  },
                  {
                    "name": "fastapi",
                    "version": null
                  },
                  {
                    "name": "lfx",
                    "version": null
                  }
                ],
                "total_dependencies": 3
              },
              "module": "custom_components.chat_output"
            },
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "loop_types": null,
                "method": "message_response",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Basic Clean Data",
                "dynamic": false,
                "info": "Whether to clean data before converting to string.",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom lfx.base.io.chat import ChatComponent\nfrom lfx.helpers.data import safe_convert\nfrom lfx.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom lfx.schema.data import Data\nfrom lfx.schema.dataframe import DataFrame\nfrom lfx.schema.message import Message\nfrom lfx.schema.properties import Source\nfrom lfx.template.field.base import Output\nfrom lfx.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/chat-input-and-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"context_id\",\n            display_name=\"Context ID\",\n            info=\"The context ID of the chat. Adds an extra layer to the local memory.\",\n            value=\"\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            advanced=True,\n            info=\"Whether to clean data before converting to string.\",\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, _, display_name, source_id = self.get_properties_from_source_component()\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message) and not self.is_connected_to_chat_input():\n            message = self.input_value\n            # Update message properties\n            message.text = text\n            # Preserve existing session_id from the incoming message if it exists\n            existing_session_id = message.session_id\n        else:\n            message = Message(text=text)\n            existing_session_id = None\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        # Preserve session_id from incoming message, or use component/graph session_id\n        message.session_id = (\n            self.session_id or existing_session_id or (self.graph.session_id if hasattr(self, \"graph\") else None) or \"\"\n        )\n        message.context_id = self.context_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n\n        # Store message if needed\n        if message.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            clean_data: bool = getattr(self, \"clean_data\", False)\n            return \"\\n\".join([safe_convert(item, clean_data=clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "context_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Context ID",
                "dynamic": false,
                "info": "The context ID of the chat. Adds an extra layer to the local memory.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "context_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "height": 234,
        "id": "ChatOutput-B13zu",
        "measured": {
          "height": 234,
          "width": 320
        },
        "position": {
          "x": 2738.611008351098,
          "y": 829.6219994149209
        },
        "positionAbsolute": {
          "x": 2734.385670401691,
          "y": 810.6079786425926
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "description": "Generate embeddings using OpenAI models.",
          "display_name": "OpenAI Embeddings",
          "id": "OpenAIEmbeddings-74cee",
          "node": {
            "base_classes": [
              "Embeddings"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate embeddings using OpenAI models.",
            "display_name": "OpenAI Embeddings",
            "documentation": "",
            "edited": false,
            "field_order": [
              "default_headers",
              "default_query",
              "chunk_size",
              "client",
              "deployment",
              "embedding_ctx_length",
              "max_retries",
              "model",
              "model_kwargs",
              "openai_api_key",
              "openai_api_base",
              "openai_api_type",
              "openai_api_version",
              "openai_organization",
              "openai_proxy",
              "request_timeout",
              "show_progress_bar",
              "skip_empty",
              "tiktoken_model_name",
              "tiktoken_enable",
              "dimensions"
            ],
            "frozen": false,
            "icon": "OpenAI",
            "legacy": false,
            "lf_version": "1.7.3",
            "metadata": {
              "code_hash": "8a658ed6d4c9",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "langchain_openai",
                    "version": null
                  },
                  {
                    "name": "lfx",
                    "version": null
                  }
                ],
                "total_dependencies": 2
              },
              "module": "custom_components.openai_embeddings"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Embedding Model",
                "group_outputs": false,
                "loop_types": null,
                "method": "build_embeddings",
                "name": "embeddings",
                "options": null,
                "required_inputs": null,
                "selected": "Embeddings",
                "tool_mode": true,
                "types": [
                  "Embeddings"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "chunk_size": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Chunk Size",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "chunk_size",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 1000
              },
              "client": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Client",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "client",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langchain_openai import OpenAIEmbeddings\n\nfrom lfx.base.embeddings.model import LCEmbeddingsModel\nfrom lfx.base.models.openai_constants import OPENAI_EMBEDDING_MODEL_NAMES\nfrom lfx.field_typing import Embeddings\nfrom lfx.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageTextInput, SecretStrInput\n\n\nclass OpenAIEmbeddingsComponent(LCEmbeddingsModel):\n    display_name = \"OpenAI Embeddings\"\n    description = \"Generate embeddings using OpenAI models.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIEmbeddings\"\n\n    inputs = [\n        DictInput(\n            name=\"default_headers\",\n            display_name=\"Default Headers\",\n            advanced=True,\n            info=\"Default headers to use for the API request.\",\n        ),\n        DictInput(\n            name=\"default_query\",\n            display_name=\"Default Query\",\n            advanced=True,\n            info=\"Default query parameters to use for the API request.\",\n        ),\n        IntInput(name=\"chunk_size\", display_name=\"Chunk Size\", advanced=True, value=1000),\n        MessageTextInput(name=\"client\", display_name=\"Client\", advanced=True),\n        MessageTextInput(name=\"deployment\", display_name=\"Deployment\", advanced=True),\n        IntInput(name=\"embedding_ctx_length\", display_name=\"Embedding Context Length\", advanced=True, value=1536),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", value=3, advanced=True),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model\",\n            advanced=False,\n            options=OPENAI_EMBEDDING_MODEL_NAMES,\n            value=\"text-embedding-3-small\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        SecretStrInput(name=\"openai_api_key\", display_name=\"OpenAI API Key\", value=\"OPENAI_API_KEY\", required=True),\n        MessageTextInput(name=\"openai_api_base\", display_name=\"OpenAI API Base\", advanced=True),\n        MessageTextInput(name=\"openai_api_type\", display_name=\"OpenAI API Type\", advanced=True),\n        MessageTextInput(name=\"openai_api_version\", display_name=\"OpenAI API Version\", advanced=True),\n        MessageTextInput(\n            name=\"openai_organization\",\n            display_name=\"OpenAI Organization\",\n            advanced=True,\n        ),\n        MessageTextInput(name=\"openai_proxy\", display_name=\"OpenAI Proxy\", advanced=True),\n        FloatInput(name=\"request_timeout\", display_name=\"Request Timeout\", advanced=True),\n        BoolInput(name=\"show_progress_bar\", display_name=\"Show Progress Bar\", advanced=True),\n        BoolInput(name=\"skip_empty\", display_name=\"Skip Empty\", advanced=True),\n        MessageTextInput(\n            name=\"tiktoken_model_name\",\n            display_name=\"TikToken Model Name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tiktoken_enable\",\n            display_name=\"TikToken Enable\",\n            advanced=True,\n            value=True,\n            info=\"If False, you must have transformers installed.\",\n        ),\n        IntInput(\n            name=\"dimensions\",\n            display_name=\"Dimensions\",\n            info=\"The number of dimensions the resulting output embeddings should have. \"\n            \"Only supported by certain models.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        return OpenAIEmbeddings(\n            client=self.client or None,\n            model=self.model,\n            dimensions=self.dimensions or None,\n            deployment=self.deployment or None,\n            api_version=self.openai_api_version or None,\n            base_url=self.openai_api_base or None,\n            openai_api_type=self.openai_api_type or None,\n            openai_proxy=self.openai_proxy or None,\n            embedding_ctx_length=self.embedding_ctx_length,\n            api_key=self.openai_api_key or None,\n            organization=self.openai_organization or None,\n            allowed_special=\"all\",\n            disallowed_special=\"all\",\n            chunk_size=self.chunk_size,\n            max_retries=self.max_retries,\n            timeout=self.request_timeout or None,\n            tiktoken_enabled=self.tiktoken_enable,\n            tiktoken_model_name=self.tiktoken_model_name or None,\n            show_progress_bar=self.show_progress_bar,\n            model_kwargs=self.model_kwargs,\n            skip_empty=self.skip_empty,\n            default_headers=self.default_headers or None,\n            default_query=self.default_query or None,\n        )\n"
              },
              "default_headers": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Default Headers",
                "dynamic": false,
                "info": "Default headers to use for the API request.",
                "list": false,
                "list_add_label": "Add More",
                "name": "default_headers",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "track_in_telemetry": false,
                "type": "dict",
                "value": {}
              },
              "default_query": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Default Query",
                "dynamic": false,
                "info": "Default query parameters to use for the API request.",
                "list": false,
                "list_add_label": "Add More",
                "name": "default_query",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "track_in_telemetry": false,
                "type": "dict",
                "value": {}
              },
              "deployment": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Deployment",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "deployment",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "dimensions": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Dimensions",
                "dynamic": false,
                "info": "The number of dimensions the resulting output embeddings should have. Only supported by certain models.",
                "list": false,
                "list_add_label": "Add More",
                "name": "dimensions",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": ""
              },
              "embedding_ctx_length": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Embedding Context Length",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "embedding_ctx_length",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 1536
              },
              "max_retries": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Retries",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_retries",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 3
              },
              "model": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "model",
                "options": [
                  "text-embedding-3-small",
                  "text-embedding-3-large",
                  "text-embedding-ada-002"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "text-embedding-3-small"
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "track_in_telemetry": false,
                "type": "dict",
                "value": {}
              },
              "openai_api_base": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "OpenAI API Base",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "openai_api_base",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "openai_api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": false,
                "name": "openai_api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "openai_api_type": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "OpenAI API Type",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "openai_api_type",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "openai_api_version": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "OpenAI API Version",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "openai_api_version",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "openai_organization": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "OpenAI Organization",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "openai_organization",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "openai_proxy": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "OpenAI Proxy",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "openai_proxy",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "request_timeout": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Request Timeout",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "request_timeout",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "float",
                "value": ""
              },
              "show_progress_bar": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Show Progress Bar",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "show_progress_bar",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": false
              },
              "skip_empty": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Skip Empty",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "skip_empty",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": false
              },
              "tiktoken_enable": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "TikToken Enable",
                "dynamic": false,
                "info": "If False, you must have transformers installed.",
                "list": false,
                "list_add_label": "Add More",
                "name": "tiktoken_enable",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              },
              "tiktoken_model_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "TikToken Model Name",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tiktoken_model_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "embeddings",
          "type": "OpenAIEmbeddings"
        },
        "dragging": false,
        "height": 320,
        "id": "OpenAIEmbeddings-74cee",
        "measured": {
          "height": 320,
          "width": 320
        },
        "position": {
          "x": 1199.146639961446,
          "y": 131.31767026474503
        },
        "positionAbsolute": {
          "x": 825.435626932521,
          "y": 739.6327999745448
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "id": "parser-DpXp4",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "category": "processing",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Format a DataFrame or Data object into text using a template. Enable 'Stringify' to convert input into a readable string instead.",
            "display_name": "Parser",
            "documentation": "",
            "edited": false,
            "field_order": [
              "mode",
              "pattern",
              "input_data",
              "sep"
            ],
            "frozen": false,
            "icon": "braces",
            "key": "parser",
            "last_updated": "2025-12-10T09:17:25.266Z",
            "legacy": false,
            "lf_version": "1.6.9",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Parsed Text",
                "group_outputs": false,
                "hidden": false,
                "method": "parse_combined_text",
                "name": "parsed_text",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 2.220446049250313e-16,
            "template": {
              "_type": "Component",
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Clean Data",
                "dynamic": false,
                "info": "Enable to clean the data by removing empty rows and lines in each cell of the DataFrame/ Data object.",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import json\nfrom typing import Any\n\nfrom langflow.custom import Component\nfrom langflow.io import (\n    BoolInput,\n    HandleInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n    TabInput,\n)\nfrom langflow.schema import Data, DataFrame\nfrom langflow.schema.message import Message\n\n\nclass ParserComponent(Component):\n    name = \"parser\"\n    display_name = \"Parser\"\n    description = (\n        \"Format a DataFrame or Data object into text using a template. \"\n        \"Enable 'Stringify' to convert input into a readable string instead.\"\n    )\n    icon = \"braces\"\n\n    inputs = [\n        TabInput(\n            name=\"mode\",\n            display_name=\"Mode\",\n            options=[\"Parser\", \"Stringify\"],\n            value=\"Parser\",\n            info=\"Convert into raw string instead of using a template.\",\n            real_time_refresh=True,\n        ),\n        MultilineInput(\n            name=\"pattern\",\n            display_name=\"Template\",\n            info=(\n                \"Use variables within curly brackets to extract column values for DataFrames \"\n                \"or key values for Data.\"\n                \"For example: `Name: {Name}, Age: {Age}, Country: {Country}`\"\n            ),\n            value=\"Text: {text}\",  # Example default\n            dynamic=True,\n            show=True,\n            required=True,\n        ),\n        HandleInput(\n            name=\"input_data\",\n            display_name=\"Data or DataFrame\",\n            input_types=[\"DataFrame\", \"Data\"],\n            info=\"Accepts either a DataFrame or a Data object.\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"sep\",\n            display_name=\"Separator\",\n            advanced=True,\n            value=\"\\n\",\n            info=\"String used to separate rows/items.\",\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Parsed Text\",\n            name=\"parsed_text\",\n            info=\"Formatted text output.\",\n            method=\"parse_combined_text\",\n        ),\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name=None):\n        \"\"\"Dynamically hide/show `template` and enforce requirement based on `stringify`.\"\"\"\n        if field_name == \"mode\":\n            build_config[\"pattern\"][\"show\"] = self.mode == \"Parser\"\n            build_config[\"pattern\"][\"required\"] = self.mode == \"Parser\"\n            if field_value:\n                clean_data = BoolInput(\n                    name=\"clean_data\",\n                    display_name=\"Clean Data\",\n                    info=(\n                        \"Enable to clean the data by removing empty rows and lines \"\n                        \"in each cell of the DataFrame/ Data object.\"\n                    ),\n                    value=True,\n                    advanced=True,\n                    required=False,\n                )\n                build_config[\"clean_data\"] = clean_data.to_dict()\n            else:\n                build_config.pop(\"clean_data\", None)\n\n        return build_config\n\n    def _clean_args(self):\n        \"\"\"Prepare arguments based on input type.\"\"\"\n        input_data = self.input_data\n\n        match input_data:\n            case list() if all(isinstance(item, Data) for item in input_data):\n                msg = \"List of Data objects is not supported.\"\n                raise ValueError(msg)\n            case DataFrame():\n                return input_data, None\n            case Data():\n                return None, input_data\n            case dict() if \"data\" in input_data:\n                try:\n                    if \"columns\" in input_data:  # Likely a DataFrame\n                        return DataFrame.from_dict(input_data), None\n                    # Likely a Data object\n                    return None, Data(**input_data)\n                except (TypeError, ValueError, KeyError) as e:\n                    msg = f\"Invalid structured input provided: {e!s}\"\n                    raise ValueError(msg) from e\n            case _:\n                msg = f\"Unsupported input type: {type(input_data)}. Expected DataFrame or Data.\"\n                raise ValueError(msg)\n\n    def parse_combined_text(self) -> Message:\n        \"\"\"Parse all rows/items into a single text or convert input to string if `stringify` is enabled.\"\"\"\n        # Early return for stringify option\n        if self.mode == \"Stringify\":\n            return self.convert_to_string()\n\n        df, data = self._clean_args()\n\n        lines = []\n        if df is not None:\n            for _, row in df.iterrows():\n                formatted_text = self.pattern.format(**row.to_dict())\n                lines.append(formatted_text)\n        elif data is not None:\n            formatted_text = self.pattern.format(**data.data)\n            lines.append(formatted_text)\n\n        combined_text = self.sep.join(lines)\n        self.status = combined_text\n        return Message(text=combined_text)\n\n    def _safe_convert(self, data: Any) -> str:\n        \"\"\"Safely convert input data to string.\"\"\"\n        try:\n            if isinstance(data, str):\n                return data\n            if isinstance(data, Message):\n                return data.get_text()\n            if isinstance(data, Data):\n                return json.dumps(data.data)\n            if isinstance(data, DataFrame):\n                if hasattr(self, \"clean_data\") and self.clean_data:\n                    # Remove empty rows\n                    data = data.dropna(how=\"all\")\n                    # Remove empty lines in each cell\n                    data = data.replace(r\"^\\s*$\", \"\", regex=True)\n                    # Replace multiple newlines with a single newline\n                    data = data.replace(r\"\\n+\", \"\\n\", regex=True)\n                return data.to_markdown(index=False)\n            return str(data)\n        except (ValueError, TypeError, AttributeError) as e:\n            msg = f\"Error converting data: {e!s}\"\n            raise ValueError(msg) from e\n\n    def convert_to_string(self) -> Message:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        result = \"\"\n        if isinstance(self.input_data, list):\n            result = \"\\n\".join([self._safe_convert(item) for item in self.input_data])\n        else:\n            result = self._safe_convert(self.input_data)\n        self.log(f\"Converted to string with length: {len(result)}\")\n\n        message = Message(text=result)\n        self.status = message\n        return message\n"
              },
              "input_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Data or DataFrame",
                "dynamic": false,
                "info": "Accepts either a DataFrame or a Data object.",
                "input_types": [
                  "DataFrame",
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_data",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "mode": {
                "_input_type": "TabInput",
                "advanced": false,
                "display_name": "Mode",
                "dynamic": false,
                "info": "Convert into raw string instead of using a template.",
                "name": "mode",
                "options": [
                  "Parser",
                  "Stringify"
                ],
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "tab",
                "value": "Stringify"
              },
              "pattern": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Template",
                "dynamic": true,
                "info": "Use variables within curly brackets to extract column values for DataFrames or key values for Data.For example: `Name: {Name}, Age: {Age}, Country: {Country}`",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "pattern",
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Text: {text}"
              },
              "sep": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "String used to separate rows/items.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sep",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n"
              }
            },
            "tool_mode": false
          },
          "selected_output": "parsed_text",
          "showNode": true,
          "type": "parser"
        },
        "dragging": false,
        "id": "parser-DpXp4",
        "measured": {
          "height": 277,
          "width": 320
        },
        "position": {
          "x": 1558.4792490573125,
          "y": 636.7212746747798
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Runs a language model given a specified provider.",
          "display_name": "Language Model",
          "id": "LanguageModelComponent-dNecf",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Runs a language model given a specified provider.",
            "display_name": "Language Model",
            "documentation": "https://docs.langflow.org/components-models",
            "edited": false,
            "field_order": [
              "provider",
              "model_name",
              "api_key",
              "base_url_ibm_watsonx",
              "project_id",
              "ollama_base_url",
              "input_value",
              "system_message",
              "stream",
              "temperature"
            ],
            "frozen": false,
            "icon": "brain-circuit",
            "last_updated": "2026-02-02T15:14:37.328Z",
            "legacy": false,
            "metadata": {
              "code_hash": "694ffc4b17b8",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "requests",
                    "version": "2.32.5"
                  },
                  {
                    "name": "langchain_anthropic",
                    "version": null
                  },
                  {
                    "name": "langchain_ibm",
                    "version": null
                  },
                  {
                    "name": "langchain_ollama",
                    "version": null
                  },
                  {
                    "name": "langchain_openai",
                    "version": null
                  },
                  {
                    "name": "pydantic",
                    "version": null
                  },
                  {
                    "name": "lfx",
                    "version": null
                  }
                ],
                "total_dependencies": 7
              },
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ],
              "module": "custom_components.language_model"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "loop_types": null,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "loop_types": null,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "selected": "LanguageModel",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": 0,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "Model Provider API key",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "base_url_ibm_watsonx": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "watsonx API Endpoint",
                "dynamic": false,
                "external_options": {},
                "info": "The base URL of the API (IBM watsonx.ai only)",
                "name": "base_url_ibm_watsonx",
                "options": [
                  "https://us-south.ml.cloud.ibm.com",
                  "https://eu-de.ml.cloud.ibm.com",
                  "https://eu-gb.ml.cloud.ibm.com",
                  "https://au-syd.ml.cloud.ibm.com",
                  "https://jp-tok.ml.cloud.ibm.com",
                  "https://ca-tor.ml.cloud.ibm.com"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "https://us-south.ml.cloud.ibm.com"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\n\nimport requests\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_ibm import ChatWatsonx\nfrom langchain_ollama import ChatOllama\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom lfx.base.models.anthropic_constants import ANTHROPIC_MODELS\nfrom lfx.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom lfx.base.models.google_generative_ai_model import ChatGoogleGenerativeAIFixed\nfrom lfx.base.models.model import LCModelComponent\nfrom lfx.base.models.model_utils import get_ollama_models, is_valid_ollama_url\nfrom lfx.base.models.openai_constants import OPENAI_CHAT_MODEL_NAMES, OPENAI_REASONING_MODEL_NAMES\nfrom lfx.field_typing import LanguageModel\nfrom lfx.field_typing.range_spec import RangeSpec\nfrom lfx.inputs.inputs import BoolInput, MessageTextInput, StrInput\nfrom lfx.io import DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\nfrom lfx.log.logger import logger\nfrom lfx.schema.dotdict import dotdict\nfrom lfx.utils.util import transform_localhost_url\n\n# IBM watsonx.ai constants\nIBM_WATSONX_DEFAULT_MODELS = [\"ibm/granite-3-2b-instruct\", \"ibm/granite-3-8b-instruct\", \"ibm/granite-13b-instruct-v2\"]\nIBM_WATSONX_URLS = [\n    \"https://us-south.ml.cloud.ibm.com\",\n    \"https://eu-de.ml.cloud.ibm.com\",\n    \"https://eu-gb.ml.cloud.ibm.com\",\n    \"https://au-syd.ml.cloud.ibm.com\",\n    \"https://jp-tok.ml.cloud.ibm.com\",\n    \"https://ca-tor.ml.cloud.ibm.com\",\n]\n\n# Ollama API constants\nHTTP_STATUS_OK = 200\nJSON_MODELS_KEY = \"models\"\nJSON_NAME_KEY = \"name\"\nJSON_CAPABILITIES_KEY = \"capabilities\"\nDESIRED_CAPABILITY = \"completion\"\nDEFAULT_OLLAMA_URL = \"http://localhost:11434\"\n\n\nclass LanguageModelComponent(LCModelComponent):\n    display_name = \"Language Model\"\n    description = \"Runs a language model given a specified provider.\"\n    documentation: str = \"https://docs.langflow.org/components-models\"\n    icon = \"brain-circuit\"\n    category = \"models\"\n    priority = 0  # Set priority to 0 to make it appear first\n\n    @staticmethod\n    def fetch_ibm_models(base_url: str) -> list[str]:\n        \"\"\"Fetch available models from the watsonx.ai API.\"\"\"\n        try:\n            endpoint = f\"{base_url}/ml/v1/foundation_model_specs\"\n            params = {\"version\": \"2024-09-16\", \"filters\": \"function_text_chat,!lifecycle_withdrawn\"}\n            response = requests.get(endpoint, params=params, timeout=10)\n            response.raise_for_status()\n            data = response.json()\n            models = [model[\"model_id\"] for model in data.get(\"resources\", [])]\n            return sorted(models)\n        except Exception:  # noqa: BLE001\n            logger.exception(\"Error fetching IBM watsonx models. Using default models.\")\n            return IBM_WATSONX_DEFAULT_MODELS\n\n    inputs = [\n        DropdownInput(\n            name=\"provider\",\n            display_name=\"Model Provider\",\n            options=[\"OpenAI\", \"Anthropic\", \"Google\", \"IBM watsonx.ai\", \"Ollama\"],\n            value=\"OpenAI\",\n            info=\"Select the model provider\",\n            real_time_refresh=True,\n            options_metadata=[\n                {\"icon\": \"OpenAI\"},\n                {\"icon\": \"Anthropic\"},\n                {\"icon\": \"GoogleGenerativeAI\"},\n                {\"icon\": \"WatsonxAI\"},\n                {\"icon\": \"Ollama\"},\n            ],\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=OPENAI_CHAT_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES,\n            value=OPENAI_CHAT_MODEL_NAMES[0],\n            info=\"Select the model to use\",\n            real_time_refresh=True,\n            refresh_button=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"Model Provider API key\",\n            required=False,\n            show=True,\n            real_time_refresh=True,\n        ),\n        DropdownInput(\n            name=\"base_url_ibm_watsonx\",\n            display_name=\"watsonx API Endpoint\",\n            info=\"The base URL of the API (IBM watsonx.ai only)\",\n            options=IBM_WATSONX_URLS,\n            value=IBM_WATSONX_URLS[0],\n            show=False,\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"project_id\",\n            display_name=\"watsonx Project ID\",\n            info=\"The project ID associated with the foundation model (IBM watsonx.ai only)\",\n            show=False,\n            required=False,\n        ),\n        MessageTextInput(\n            name=\"ollama_base_url\",\n            display_name=\"Ollama API URL\",\n            info=f\"Endpoint of the Ollama API (Ollama only). Defaults to {DEFAULT_OLLAMA_URL}\",\n            value=DEFAULT_OLLAMA_URL,\n            show=False,\n            real_time_refresh=True,\n            load_from_db=True,\n        ),\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n            info=\"The input text to send to the model\",\n        ),\n        MultilineInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"A system message that helps set the behavior of the assistant\",\n            advanced=False,\n        ),\n        BoolInput(\n            name=\"stream\",\n            display_name=\"Stream\",\n            info=\"Whether to stream the response\",\n            value=False,\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            info=\"Controls randomness in responses\",\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:\n        provider = self.provider\n        model_name = self.model_name\n        temperature = self.temperature\n        stream = self.stream\n\n        if provider == \"OpenAI\":\n            if not self.api_key:\n                msg = \"OpenAI API key is required when using OpenAI provider\"\n                raise ValueError(msg)\n\n            if model_name in OPENAI_REASONING_MODEL_NAMES:\n                # reasoning models do not support temperature (yet)\n                temperature = None\n\n            return ChatOpenAI(\n                model_name=model_name,\n                temperature=temperature,\n                streaming=stream,\n                openai_api_key=self.api_key,\n            )\n        if provider == \"Anthropic\":\n            if not self.api_key:\n                msg = \"Anthropic API key is required when using Anthropic provider\"\n                raise ValueError(msg)\n            return ChatAnthropic(\n                model=model_name,\n                temperature=temperature,\n                streaming=stream,\n                anthropic_api_key=self.api_key,\n            )\n        if provider == \"Google\":\n            if not self.api_key:\n                msg = \"Google API key is required when using Google provider\"\n                raise ValueError(msg)\n            return ChatGoogleGenerativeAIFixed(\n                model=model_name,\n                temperature=temperature,\n                streaming=stream,\n                google_api_key=self.api_key,\n            )\n        if provider == \"IBM watsonx.ai\":\n            if not self.api_key:\n                msg = \"IBM API key is required when using IBM watsonx.ai provider\"\n                raise ValueError(msg)\n            if not self.base_url_ibm_watsonx:\n                msg = \"IBM watsonx API Endpoint is required when using IBM watsonx.ai provider\"\n                raise ValueError(msg)\n            if not self.project_id:\n                msg = \"IBM watsonx Project ID is required when using IBM watsonx.ai provider\"\n                raise ValueError(msg)\n            return ChatWatsonx(\n                apikey=SecretStr(self.api_key).get_secret_value(),\n                url=self.base_url_ibm_watsonx,\n                project_id=self.project_id,\n                model_id=model_name,\n                params={\n                    \"temperature\": temperature,\n                },\n                streaming=stream,\n            )\n        if provider == \"Ollama\":\n            if not self.ollama_base_url:\n                msg = \"Ollama API URL is required when using Ollama provider\"\n                raise ValueError(msg)\n            if not model_name:\n                msg = \"Model name is required when using Ollama provider\"\n                raise ValueError(msg)\n\n            transformed_base_url = transform_localhost_url(self.ollama_base_url)\n\n            # Check if URL contains /v1 suffix (OpenAI-compatible mode)\n            if transformed_base_url and transformed_base_url.rstrip(\"/\").endswith(\"/v1\"):\n                # Strip /v1 suffix and log warning\n                transformed_base_url = transformed_base_url.rstrip(\"/\").removesuffix(\"/v1\")\n                logger.warning(\n                    \"Detected '/v1' suffix in base URL. The Ollama component uses the native Ollama API, \"\n                    \"not the OpenAI-compatible API. The '/v1' suffix has been automatically removed. \"\n                    \"If you want to use the OpenAI-compatible API, please use the OpenAI component instead. \"\n                    \"Learn more at https://docs.ollama.com/openai#openai-compatibility\"\n                )\n\n            return ChatOllama(\n                base_url=transformed_base_url,\n                model=model_name,\n                temperature=temperature,\n            )\n        msg = f\"Unknown provider: {provider}\"\n        raise ValueError(msg)\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: Any, field_name: str | None = None\n    ) -> dotdict:\n        if field_name == \"provider\":\n            if field_value == \"OpenAI\":\n                build_config[\"model_name\"][\"options\"] = OPENAI_CHAT_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES\n                build_config[\"model_name\"][\"value\"] = OPENAI_CHAT_MODEL_NAMES[0]\n                build_config[\"api_key\"][\"display_name\"] = \"OpenAI API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"Anthropic\":\n                build_config[\"model_name\"][\"options\"] = ANTHROPIC_MODELS\n                build_config[\"model_name\"][\"value\"] = ANTHROPIC_MODELS[0]\n                build_config[\"api_key\"][\"display_name\"] = \"Anthropic API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"Google\":\n                build_config[\"model_name\"][\"options\"] = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"value\"] = GOOGLE_GENERATIVE_AI_MODELS[0]\n                build_config[\"api_key\"][\"display_name\"] = \"Google API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"IBM watsonx.ai\":\n                build_config[\"model_name\"][\"options\"] = IBM_WATSONX_DEFAULT_MODELS\n                build_config[\"model_name\"][\"value\"] = IBM_WATSONX_DEFAULT_MODELS[0]\n                build_config[\"api_key\"][\"display_name\"] = \"IBM API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = True\n                build_config[\"project_id\"][\"show\"] = True\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"Ollama\":\n                # Fetch Ollama models from the API\n                build_config[\"api_key\"][\"show\"] = False\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = True\n\n                # Try multiple sources to get the URL (in order of preference):\n                # 1. Instance attribute (already resolved from global/db)\n                # 2. Build config value (may be a global variable reference)\n                # 3. Default value\n                ollama_url = getattr(self, \"ollama_base_url\", None)\n                if not ollama_url:\n                    config_value = build_config[\"ollama_base_url\"].get(\"value\", DEFAULT_OLLAMA_URL)\n                    # If config_value looks like a variable name (all caps with underscores), use default\n                    is_variable_ref = (\n                        config_value\n                        and isinstance(config_value, str)\n                        and config_value.isupper()\n                        and \"_\" in config_value\n                    )\n                    if is_variable_ref:\n                        await logger.adebug(\n                            f\"Config value appears to be a variable reference: {config_value}, using default\"\n                        )\n                        ollama_url = DEFAULT_OLLAMA_URL\n                    else:\n                        ollama_url = config_value\n\n                await logger.adebug(f\"Fetching Ollama models for provider switch. URL: {ollama_url}\")\n                if await is_valid_ollama_url(url=ollama_url):\n                    try:\n                        models = await get_ollama_models(\n                            base_url_value=ollama_url,\n                            desired_capability=DESIRED_CAPABILITY,\n                            json_models_key=JSON_MODELS_KEY,\n                            json_name_key=JSON_NAME_KEY,\n                            json_capabilities_key=JSON_CAPABILITIES_KEY,\n                        )\n                        build_config[\"model_name\"][\"options\"] = models\n                        build_config[\"model_name\"][\"value\"] = models[0] if models else \"\"\n                    except ValueError:\n                        await logger.awarning(\"Failed to fetch Ollama models. Setting empty options.\")\n                        build_config[\"model_name\"][\"options\"] = []\n                        build_config[\"model_name\"][\"value\"] = \"\"\n                else:\n                    await logger.awarning(f\"Invalid Ollama URL: {ollama_url}\")\n                    build_config[\"model_name\"][\"options\"] = []\n                    build_config[\"model_name\"][\"value\"] = \"\"\n        elif (\n            field_name == \"base_url_ibm_watsonx\"\n            and field_value\n            and hasattr(self, \"provider\")\n            and self.provider == \"IBM watsonx.ai\"\n        ):\n            # Fetch IBM models when base_url changes\n            try:\n                models = self.fetch_ibm_models(base_url=field_value)\n                build_config[\"model_name\"][\"options\"] = models\n                build_config[\"model_name\"][\"value\"] = models[0] if models else IBM_WATSONX_DEFAULT_MODELS[0]\n                info_message = f\"Updated model options: {len(models)} models found in {field_value}\"\n                logger.info(info_message)\n            except Exception:  # noqa: BLE001\n                logger.exception(\"Error updating IBM model options.\")\n        elif field_name == \"ollama_base_url\":\n            # Fetch Ollama models when ollama_base_url changes\n            # Use the field_value directly since this is triggered when the field changes\n            logger.debug(\n                f\"Fetching Ollama models from updated URL: {build_config['ollama_base_url']} \\\n                and value {self.ollama_base_url}\",\n            )\n            await logger.adebug(f\"Fetching Ollama models from updated URL: {self.ollama_base_url}\")\n            if await is_valid_ollama_url(url=self.ollama_base_url):\n                try:\n                    models = await get_ollama_models(\n                        base_url_value=self.ollama_base_url,\n                        desired_capability=DESIRED_CAPABILITY,\n                        json_models_key=JSON_MODELS_KEY,\n                        json_name_key=JSON_NAME_KEY,\n                        json_capabilities_key=JSON_CAPABILITIES_KEY,\n                    )\n                    build_config[\"model_name\"][\"options\"] = models\n                    build_config[\"model_name\"][\"value\"] = models[0] if models else \"\"\n                    info_message = f\"Updated model options: {len(models)} models found in {self.ollama_base_url}\"\n                    await logger.ainfo(info_message)\n                except ValueError:\n                    await logger.awarning(\"Error updating Ollama model options.\")\n                    build_config[\"model_name\"][\"options\"] = []\n                    build_config[\"model_name\"][\"value\"] = \"\"\n            else:\n                await logger.awarning(f\"Invalid Ollama URL: {self.ollama_base_url}\")\n                build_config[\"model_name\"][\"options\"] = []\n                build_config[\"model_name\"][\"value\"] = \"\"\n        elif field_name == \"model_name\":\n            # Refresh Ollama models when model_name field is accessed\n            if hasattr(self, \"provider\") and self.provider == \"Ollama\":\n                ollama_url = getattr(self, \"ollama_base_url\", DEFAULT_OLLAMA_URL)\n                if await is_valid_ollama_url(url=ollama_url):\n                    try:\n                        models = await get_ollama_models(\n                            base_url_value=ollama_url,\n                            desired_capability=DESIRED_CAPABILITY,\n                            json_models_key=JSON_MODELS_KEY,\n                            json_name_key=JSON_NAME_KEY,\n                            json_capabilities_key=JSON_CAPABILITIES_KEY,\n                        )\n                        build_config[\"model_name\"][\"options\"] = models\n                    except ValueError:\n                        await logger.awarning(\"Failed to refresh Ollama models.\")\n                        build_config[\"model_name\"][\"options\"] = []\n                else:\n                    build_config[\"model_name\"][\"options\"] = []\n\n            # Hide system_message for o1 models - currently unsupported\n            if field_value and field_value.startswith(\"o1\") and hasattr(self, \"provider\") and self.provider == \"OpenAI\":\n                if \"system_message\" in build_config:\n                    build_config[\"system_message\"][\"show\"] = False\n            elif \"system_message\" in build_config:\n                build_config[\"system_message\"][\"show\"] = True\n        return build_config\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input text to send to the model",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "external_options": {},
                "info": "Select the model to use",
                "name": "model_name",
                "options": [
                  "gpt-4o-mini",
                  "gpt-4o",
                  "gpt-4.1",
                  "gpt-4.1-mini",
                  "gpt-4.1-nano",
                  "gpt-4-turbo",
                  "gpt-4-turbo-preview",
                  "gpt-4",
                  "gpt-3.5-turbo",
                  "gpt-5.1",
                  "gpt-5",
                  "gpt-5-mini",
                  "gpt-5-nano",
                  "gpt-5-chat-latest",
                  "o1",
                  "o3-mini",
                  "o3",
                  "o3-pro",
                  "o4-mini",
                  "o4-mini-high"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "gpt-4o-mini"
              },
              "ollama_base_url": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Ollama API URL",
                "dynamic": false,
                "info": "Endpoint of the Ollama API (Ollama only). Defaults to http://localhost:11434",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "ollama_base_url",
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "project_id": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "watsonx Project ID",
                "dynamic": false,
                "info": "The project ID associated with the foundation model (IBM watsonx.ai only)",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "project_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "provider": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {},
                "info": "Select the model provider",
                "name": "provider",
                "options": [
                  "OpenAI",
                  "Anthropic",
                  "Google",
                  "IBM watsonx.ai",
                  "Ollama"
                ],
                "options_metadata": [
                  {
                    "icon": "OpenAI"
                  },
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "WatsonxAI"
                  },
                  {
                    "icon": "Ollama"
                  }
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "OpenAI"
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Whether to stream the response",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "A system message that helps set the behavior of the assistant",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "SliderInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "Controls randomness in responses",
                "max_label": "",
                "max_label_icon": "",
                "min_label": "",
                "min_label_icon": "",
                "name": "temperature",
                "override_skip": false,
                "placeholder": "",
                "range_spec": {
                  "max": 1,
                  "min": 0,
                  "step": 0.01,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "title_case": false,
                "tool_mode": false,
                "track_in_telemetry": false,
                "type": "slider",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "LanguageModelComponent"
        },
        "dragging": false,
        "id": "LanguageModelComponent-dNecf",
        "measured": {
          "height": 530,
          "width": 320
        },
        "position": {
          "x": 2354.7612483129965,
          "y": 633.8261067248878
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "OpenSearch-M8Kna",
          "node": {
            "base_classes": [
              "Data",
              "DataFrame",
              "VectorStore"
            ],
            "beta": false,
            "category": "elastic",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "OpenSearch Vector Store with advanced, customizable search capabilities.",
            "display_name": "OpenSearch",
            "documentation": "",
            "edited": false,
            "field_order": [
              "opensearch_url",
              "index_name",
              "ingest_data",
              "search_query",
              "should_cache_vector_store",
              "embedding",
              "search_type",
              "number_of_results",
              "search_score_threshold",
              "username",
              "password",
              "use_ssl",
              "verify_certs",
              "hybrid_search_query"
            ],
            "frozen": false,
            "icon": "OpenSearch",
            "key": "OpenSearch",
            "legacy": false,
            "lf_version": "1.6.9",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Search Results",
                "group_outputs": false,
                "method": "search_documents",
                "name": "search_results",
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "DataFrame",
                "group_outputs": false,
                "method": "as_dataframe",
                "name": "dataframe",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Vector Store Connection",
                "group_outputs": false,
                "hidden": false,
                "method": "as_vector_store",
                "name": "vectorstoreconnection",
                "tool_mode": true,
                "types": [
                  "VectorStore"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.00008669618757582168,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import json\nfrom typing import Any\n\nfrom langchain_community.vectorstores import OpenSearchVectorSearch\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.base.vectorstores.vector_store_connection_decorator import vector_store_connection\nfrom langflow.io import (\n    BoolInput,\n    DropdownInput,\n    FloatInput,\n    HandleInput,\n    IntInput,\n    MultilineInput,\n    SecretStrInput,\n    StrInput,\n)\nfrom langflow.schema.data import Data\n\n\n@vector_store_connection\nclass OpenSearchVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"OpenSearch Vector Store with advanced, customizable search capabilities.\"\"\"\n\n    display_name: str = \"OpenSearch\"\n    description: str = \"OpenSearch Vector Store with advanced, customizable search capabilities.\"\n    name = \"OpenSearch\"\n    icon = \"OpenSearch\"\n\n    inputs = [\n        StrInput(\n            name=\"opensearch_url\",\n            display_name=\"OpenSearch URL\",\n            value=\"http://localhost:9200\",\n            info=\"URL for OpenSearch cluster (e.g. https://192.168.1.1:9200).\",\n        ),\n        StrInput(\n            name=\"index_name\",\n            display_name=\"Index Name\",\n            value=\"langflow\",\n            info=\"The index name where the vectors will be stored in OpenSearch cluster.\",\n        ),\n        *LCVectorStoreComponent.inputs,\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        DropdownInput(\n            name=\"search_type\",\n            display_name=\"Search Type\",\n            options=[\"similarity\", \"similarity_score_threshold\", \"mmr\"],\n            value=\"similarity\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=4,\n        ),\n        FloatInput(\n            name=\"search_score_threshold\",\n            display_name=\"Search Score Threshold\",\n            info=\"Minimum similarity score threshold for search results.\",\n            value=0.0,\n            advanced=True,\n        ),\n        StrInput(\n            name=\"username\",\n            display_name=\"Username\",\n            value=\"admin\",\n            advanced=True,\n        ),\n        SecretStrInput(\n            name=\"password\",\n            display_name=\"OpenSearch Password\",\n            value=\"admin\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"use_ssl\",\n            display_name=\"Use SSL\",\n            value=True,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verify_certs\",\n            display_name=\"Verify Certificates\",\n            value=False,\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"hybrid_search_query\",\n            display_name=\"Hybrid Search Query\",\n            value=\"\",\n            advanced=True,\n            info=(\n                \"Provide a custom hybrid search query in JSON format. This allows you to combine \"\n                \"vector similarity and keyword matching.\"\n            ),\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> OpenSearchVectorSearch:\n        \"\"\"Builds the OpenSearch Vector Store object.\"\"\"\n        try:\n            from langchain_community.vectorstores import OpenSearchVectorSearch\n        except ImportError as e:\n            error_message = f\"Failed to import required modules: {e}\"\n            self.log(error_message)\n            raise ImportError(error_message) from e\n\n        try:\n            opensearch = OpenSearchVectorSearch(\n                index_name=self.index_name,\n                embedding_function=self.embedding,\n                opensearch_url=self.opensearch_url,\n                http_auth=(self.username, self.password),\n                use_ssl=self.use_ssl,\n                verify_certs=self.verify_certs,\n                ssl_assert_hostname=False,\n                ssl_show_warn=False,\n            )\n        except Exception as e:\n            error_message = f\"Failed to create OpenSearchVectorSearch instance: {e}\"\n            self.log(error_message)\n            raise RuntimeError(error_message) from e\n\n        if self.ingest_data:\n            self._add_documents_to_vector_store(opensearch)\n\n        return opensearch\n\n    def _add_documents_to_vector_store(self, vector_store: \"OpenSearchVectorSearch\") -> None:\n        \"\"\"Adds documents to the Vector Store.\"\"\"\n        # Convert DataFrame to Data if needed using parent's method\n        self.ingest_data = self._prepare_ingest_data()\n\n        documents = []\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                error_message = f\"Expected Data object, got {type(_input)}\"\n                self.log(error_message)\n                raise TypeError(error_message)\n\n        if documents and self.embedding is not None:\n            self.log(f\"Adding {len(documents)} documents to the Vector Store.\")\n            try:\n                vector_store.add_documents(documents)\n            except Exception as e:\n                error_message = f\"Error adding documents to Vector Store: {e}\"\n                self.log(error_message)\n                raise RuntimeError(error_message) from e\n        else:\n            self.log(\"No documents to add to the Vector Store.\")\n\n    def search(self, query: str | None = None) -> list[dict[str, Any]]:\n        \"\"\"Search for similar documents in the vector store or retrieve all documents if no query is provided.\"\"\"\n        try:\n            vector_store = self.build_vector_store()\n\n            query = query or \"\"\n\n            if self.hybrid_search_query.strip():\n                try:\n                    hybrid_query = json.loads(self.hybrid_search_query)\n                except json.JSONDecodeError as e:\n                    error_message = f\"Invalid hybrid search query JSON: {e}\"\n                    self.log(error_message)\n                    raise ValueError(error_message) from e\n\n                results = vector_store.client.search(index=self.index_name, body=hybrid_query)\n\n                processed_results = []\n                for hit in results.get(\"hits\", {}).get(\"hits\", []):\n                    source = hit.get(\"_source\", {})\n                    text = source.get(\"text\", \"\")\n                    metadata = source.get(\"metadata\", {})\n\n                    if isinstance(text, dict):\n                        text = text.get(\"text\", \"\")\n\n                    processed_results.append(\n                        {\n                            \"page_content\": text,\n                            \"metadata\": metadata,\n                        }\n                    )\n                return processed_results\n\n            search_kwargs = {\"k\": self.number_of_results}\n            search_type = self.search_type.lower()\n\n            if search_type == \"similarity\":\n                results = vector_store.similarity_search(query, **search_kwargs)\n                return [{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in results]\n            if search_type == \"similarity_score_threshold\":\n                search_kwargs[\"score_threshold\"] = self.search_score_threshold\n                results = vector_store.similarity_search_with_relevance_scores(query, **search_kwargs)\n                return [\n                    {\n                        \"page_content\": doc.page_content,\n                        \"metadata\": doc.metadata,\n                        \"score\": score,\n                    }\n                    for doc, score in results\n                ]\n            if search_type == \"mmr\":\n                results = vector_store.max_marginal_relevance_search(query, **search_kwargs)\n                return [{\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in results]\n\n        except Exception as e:\n            error_message = f\"Error during search: {e}\"\n            self.log(error_message)\n            raise RuntimeError(error_message) from e\n\n        error_message = f\"Error during search. Invalid search type: {self.search_type}\"\n        self.log(error_message)\n        raise ValueError(error_message)\n\n    def search_documents(self) -> list[Data]:\n        \"\"\"Search for documents in the vector store based on the search input.\n\n        If no search input is provided, retrieve all documents.\n        \"\"\"\n        try:\n            query = self.search_query.strip() if self.search_query else None\n            results = self.search(query)\n            retrieved_data = [\n                Data(\n                    file_path=result[\"metadata\"].get(\"file_path\", \"\"),\n                    text=result[\"page_content\"],\n                )\n                for result in results\n            ]\n        except Exception as e:\n            error_message = f\"Error during document search: {e}\"\n            self.log(error_message)\n            raise RuntimeError(error_message) from e\n\n        self.status = retrieved_data\n        return retrieved_data\n"
              },
              "embedding": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Embedding",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Embeddings"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "embedding",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "hybrid_search_query": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Hybrid Search Query",
                "dynamic": false,
                "info": "Provide a custom hybrid search query in JSON format. This allows you to combine vector similarity and keyword matching.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "hybrid_search_query",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "index_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Index Name",
                "dynamic": false,
                "info": "The index name where the vectors will be stored in OpenSearch cluster.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "index_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "rag_demo"
              },
              "ingest_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Ingest Data",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data",
                  "DataFrame"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "ingest_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "number_of_results": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Number of Results",
                "dynamic": false,
                "info": "Number of results to return.",
                "list": false,
                "list_add_label": "Add More",
                "name": "number_of_results",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 2
              },
              "opensearch_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "OpenSearch URL",
                "dynamic": false,
                "info": "URL for OpenSearch cluster (e.g. https://192.168.1.1:9200).",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "opensearch_url",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "password": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenSearch Password",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": false,
                "name": "password",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "search_query": {
                "_input_type": "QueryInput",
                "advanced": false,
                "display_name": "Search Query",
                "dynamic": false,
                "info": "Enter a query to run a similarity search.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "search_query",
                "placeholder": "Enter a query...",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "query",
                "value": ""
              },
              "search_score_threshold": {
                "_input_type": "FloatInput",
                "advanced": true,
                "display_name": "Search Score Threshold",
                "dynamic": false,
                "info": "Minimum similarity score threshold for search results.",
                "list": false,
                "list_add_label": "Add More",
                "name": "search_score_threshold",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0
              },
              "search_type": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Search Type",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "search_type",
                "options": [
                  "similarity",
                  "similarity_score_threshold",
                  "mmr"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "similarity"
              },
              "should_cache_vector_store": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Cache Vector Store",
                "dynamic": false,
                "info": "If True, the vector store will be cached for the current build of the component. This is useful for components that have multiple output methods and want to share the same vector store.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_cache_vector_store",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "use_ssl": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Use SSL",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "use_ssl",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "username": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Username",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "username",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "verify_certs": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verify Certificates",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "verify_certs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              }
            },
            "tool_mode": false
          },
          "selected_output": "search_results",
          "showNode": true,
          "type": "OpenSearch"
        },
        "dragging": false,
        "id": "OpenSearch-M8Kna",
        "measured": {
          "height": 841,
          "width": 320
        },
        "position": {
          "x": 1207.2635178878081,
          "y": 523.9217466190689
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-jLz8p",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": [
                "question"
              ]
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "error": null,
            "field_order": [
              "template"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "braces",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "legacy": false,
            "lf_version": "1.7.3",
            "metadata": {
              "code_hash": "3bf0b511e227",
              "module": "langflow.components.prompts.prompt.PromptComponent"
            },
            "minimized": false,
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt",
                "group_outputs": false,
                "hidden": null,
                "method": "build_prompt",
                "name": "prompt",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": null,
            "replacement": null,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "question": {
                "advanced": false,
                "display_name": "question",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "question",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "You are a keyword extractor for OpenSearch BM25 search on the LEXORA support knowledge base.\nIn this system, documents are chunked by Unstructured and stored in OpenSearch index rag_demo. Each chunkâ€™s searchable text is in the field text (do not use keywords, title, content, or any other fields).\nTask\nExtract 2â€“5 important keywords/phrases from the user question (no stop-words).\nReturn a simple BM25 query JSON that searches only the text field.\nLangflow will combine this BM25 query with vector search for hybrid retrieval.\nRules\nRemove question/stop words (e.g., what, is, are, the, a, an, how, do, does, can, my, your, tell, about).\nPrefer specific nouns/entities (product names, countries, fees, limits, â€œAPRâ€, etc.).\nOutput ONLY valid JSON (no markdown, no commentary).\nUse double braces {{ }} in the output (Langflow template-safe).\n\nQUESTION: {question}\n\nOUTPUT (JSON ONLY):\n{{\n\"query\": {{\n\"match\": {{\n\"text\": {{\n\"query\": \"<keywords here>\",\n\"operator\": \"or\"\n}}\n}}\n}},\n\"size\": 10\n}}"
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "prompt",
          "type": "Prompt"
        },
        "dragging": false,
        "id": "Prompt-jLz8p",
        "measured": {
          "height": 365,
          "width": 320
        },
        "position": {
          "x": 806.7086756201708,
          "y": 109.53460389749873
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "description": "Runs a language model given a specified provider.",
          "display_name": "Language Model",
          "id": "LanguageModelComponent-JQa1y",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Runs a language model given a specified provider.",
            "display_name": "Language Model",
            "documentation": "https://docs.langflow.org/components-models",
            "edited": false,
            "field_order": [
              "provider",
              "model_name",
              "api_key",
              "base_url_ibm_watsonx",
              "project_id",
              "ollama_base_url",
              "input_value",
              "system_message",
              "stream",
              "temperature"
            ],
            "frozen": false,
            "icon": "brain-circuit",
            "last_updated": "2026-02-02T15:14:37.329Z",
            "legacy": false,
            "lf_version": "1.7.3",
            "metadata": {
              "code_hash": "694ffc4b17b8",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "requests",
                    "version": "2.32.5"
                  },
                  {
                    "name": "langchain_anthropic",
                    "version": null
                  },
                  {
                    "name": "langchain_ibm",
                    "version": null
                  },
                  {
                    "name": "langchain_ollama",
                    "version": null
                  },
                  {
                    "name": "langchain_openai",
                    "version": null
                  },
                  {
                    "name": "pydantic",
                    "version": null
                  },
                  {
                    "name": "lfx",
                    "version": null
                  }
                ],
                "total_dependencies": 7
              },
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ],
              "module": "custom_components.language_model"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "loop_types": null,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "loop_types": null,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "selected": "LanguageModel",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": 0,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "Model Provider API key",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "base_url_ibm_watsonx": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "watsonx API Endpoint",
                "dynamic": false,
                "external_options": {},
                "info": "The base URL of the API (IBM watsonx.ai only)",
                "name": "base_url_ibm_watsonx",
                "options": [
                  "https://us-south.ml.cloud.ibm.com",
                  "https://eu-de.ml.cloud.ibm.com",
                  "https://eu-gb.ml.cloud.ibm.com",
                  "https://au-syd.ml.cloud.ibm.com",
                  "https://jp-tok.ml.cloud.ibm.com",
                  "https://ca-tor.ml.cloud.ibm.com"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "https://us-south.ml.cloud.ibm.com"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\n\nimport requests\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_ibm import ChatWatsonx\nfrom langchain_ollama import ChatOllama\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom lfx.base.models.anthropic_constants import ANTHROPIC_MODELS\nfrom lfx.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom lfx.base.models.google_generative_ai_model import ChatGoogleGenerativeAIFixed\nfrom lfx.base.models.model import LCModelComponent\nfrom lfx.base.models.model_utils import get_ollama_models, is_valid_ollama_url\nfrom lfx.base.models.openai_constants import OPENAI_CHAT_MODEL_NAMES, OPENAI_REASONING_MODEL_NAMES\nfrom lfx.field_typing import LanguageModel\nfrom lfx.field_typing.range_spec import RangeSpec\nfrom lfx.inputs.inputs import BoolInput, MessageTextInput, StrInput\nfrom lfx.io import DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\nfrom lfx.log.logger import logger\nfrom lfx.schema.dotdict import dotdict\nfrom lfx.utils.util import transform_localhost_url\n\n# IBM watsonx.ai constants\nIBM_WATSONX_DEFAULT_MODELS = [\"ibm/granite-3-2b-instruct\", \"ibm/granite-3-8b-instruct\", \"ibm/granite-13b-instruct-v2\"]\nIBM_WATSONX_URLS = [\n    \"https://us-south.ml.cloud.ibm.com\",\n    \"https://eu-de.ml.cloud.ibm.com\",\n    \"https://eu-gb.ml.cloud.ibm.com\",\n    \"https://au-syd.ml.cloud.ibm.com\",\n    \"https://jp-tok.ml.cloud.ibm.com\",\n    \"https://ca-tor.ml.cloud.ibm.com\",\n]\n\n# Ollama API constants\nHTTP_STATUS_OK = 200\nJSON_MODELS_KEY = \"models\"\nJSON_NAME_KEY = \"name\"\nJSON_CAPABILITIES_KEY = \"capabilities\"\nDESIRED_CAPABILITY = \"completion\"\nDEFAULT_OLLAMA_URL = \"http://localhost:11434\"\n\n\nclass LanguageModelComponent(LCModelComponent):\n    display_name = \"Language Model\"\n    description = \"Runs a language model given a specified provider.\"\n    documentation: str = \"https://docs.langflow.org/components-models\"\n    icon = \"brain-circuit\"\n    category = \"models\"\n    priority = 0  # Set priority to 0 to make it appear first\n\n    @staticmethod\n    def fetch_ibm_models(base_url: str) -> list[str]:\n        \"\"\"Fetch available models from the watsonx.ai API.\"\"\"\n        try:\n            endpoint = f\"{base_url}/ml/v1/foundation_model_specs\"\n            params = {\"version\": \"2024-09-16\", \"filters\": \"function_text_chat,!lifecycle_withdrawn\"}\n            response = requests.get(endpoint, params=params, timeout=10)\n            response.raise_for_status()\n            data = response.json()\n            models = [model[\"model_id\"] for model in data.get(\"resources\", [])]\n            return sorted(models)\n        except Exception:  # noqa: BLE001\n            logger.exception(\"Error fetching IBM watsonx models. Using default models.\")\n            return IBM_WATSONX_DEFAULT_MODELS\n\n    inputs = [\n        DropdownInput(\n            name=\"provider\",\n            display_name=\"Model Provider\",\n            options=[\"OpenAI\", \"Anthropic\", \"Google\", \"IBM watsonx.ai\", \"Ollama\"],\n            value=\"OpenAI\",\n            info=\"Select the model provider\",\n            real_time_refresh=True,\n            options_metadata=[\n                {\"icon\": \"OpenAI\"},\n                {\"icon\": \"Anthropic\"},\n                {\"icon\": \"GoogleGenerativeAI\"},\n                {\"icon\": \"WatsonxAI\"},\n                {\"icon\": \"Ollama\"},\n            ],\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=OPENAI_CHAT_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES,\n            value=OPENAI_CHAT_MODEL_NAMES[0],\n            info=\"Select the model to use\",\n            real_time_refresh=True,\n            refresh_button=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"Model Provider API key\",\n            required=False,\n            show=True,\n            real_time_refresh=True,\n        ),\n        DropdownInput(\n            name=\"base_url_ibm_watsonx\",\n            display_name=\"watsonx API Endpoint\",\n            info=\"The base URL of the API (IBM watsonx.ai only)\",\n            options=IBM_WATSONX_URLS,\n            value=IBM_WATSONX_URLS[0],\n            show=False,\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"project_id\",\n            display_name=\"watsonx Project ID\",\n            info=\"The project ID associated with the foundation model (IBM watsonx.ai only)\",\n            show=False,\n            required=False,\n        ),\n        MessageTextInput(\n            name=\"ollama_base_url\",\n            display_name=\"Ollama API URL\",\n            info=f\"Endpoint of the Ollama API (Ollama only). Defaults to {DEFAULT_OLLAMA_URL}\",\n            value=DEFAULT_OLLAMA_URL,\n            show=False,\n            real_time_refresh=True,\n            load_from_db=True,\n        ),\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n            info=\"The input text to send to the model\",\n        ),\n        MultilineInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"A system message that helps set the behavior of the assistant\",\n            advanced=False,\n        ),\n        BoolInput(\n            name=\"stream\",\n            display_name=\"Stream\",\n            info=\"Whether to stream the response\",\n            value=False,\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            info=\"Controls randomness in responses\",\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:\n        provider = self.provider\n        model_name = self.model_name\n        temperature = self.temperature\n        stream = self.stream\n\n        if provider == \"OpenAI\":\n            if not self.api_key:\n                msg = \"OpenAI API key is required when using OpenAI provider\"\n                raise ValueError(msg)\n\n            if model_name in OPENAI_REASONING_MODEL_NAMES:\n                # reasoning models do not support temperature (yet)\n                temperature = None\n\n            return ChatOpenAI(\n                model_name=model_name,\n                temperature=temperature,\n                streaming=stream,\n                openai_api_key=self.api_key,\n            )\n        if provider == \"Anthropic\":\n            if not self.api_key:\n                msg = \"Anthropic API key is required when using Anthropic provider\"\n                raise ValueError(msg)\n            return ChatAnthropic(\n                model=model_name,\n                temperature=temperature,\n                streaming=stream,\n                anthropic_api_key=self.api_key,\n            )\n        if provider == \"Google\":\n            if not self.api_key:\n                msg = \"Google API key is required when using Google provider\"\n                raise ValueError(msg)\n            return ChatGoogleGenerativeAIFixed(\n                model=model_name,\n                temperature=temperature,\n                streaming=stream,\n                google_api_key=self.api_key,\n            )\n        if provider == \"IBM watsonx.ai\":\n            if not self.api_key:\n                msg = \"IBM API key is required when using IBM watsonx.ai provider\"\n                raise ValueError(msg)\n            if not self.base_url_ibm_watsonx:\n                msg = \"IBM watsonx API Endpoint is required when using IBM watsonx.ai provider\"\n                raise ValueError(msg)\n            if not self.project_id:\n                msg = \"IBM watsonx Project ID is required when using IBM watsonx.ai provider\"\n                raise ValueError(msg)\n            return ChatWatsonx(\n                apikey=SecretStr(self.api_key).get_secret_value(),\n                url=self.base_url_ibm_watsonx,\n                project_id=self.project_id,\n                model_id=model_name,\n                params={\n                    \"temperature\": temperature,\n                },\n                streaming=stream,\n            )\n        if provider == \"Ollama\":\n            if not self.ollama_base_url:\n                msg = \"Ollama API URL is required when using Ollama provider\"\n                raise ValueError(msg)\n            if not model_name:\n                msg = \"Model name is required when using Ollama provider\"\n                raise ValueError(msg)\n\n            transformed_base_url = transform_localhost_url(self.ollama_base_url)\n\n            # Check if URL contains /v1 suffix (OpenAI-compatible mode)\n            if transformed_base_url and transformed_base_url.rstrip(\"/\").endswith(\"/v1\"):\n                # Strip /v1 suffix and log warning\n                transformed_base_url = transformed_base_url.rstrip(\"/\").removesuffix(\"/v1\")\n                logger.warning(\n                    \"Detected '/v1' suffix in base URL. The Ollama component uses the native Ollama API, \"\n                    \"not the OpenAI-compatible API. The '/v1' suffix has been automatically removed. \"\n                    \"If you want to use the OpenAI-compatible API, please use the OpenAI component instead. \"\n                    \"Learn more at https://docs.ollama.com/openai#openai-compatibility\"\n                )\n\n            return ChatOllama(\n                base_url=transformed_base_url,\n                model=model_name,\n                temperature=temperature,\n            )\n        msg = f\"Unknown provider: {provider}\"\n        raise ValueError(msg)\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: Any, field_name: str | None = None\n    ) -> dotdict:\n        if field_name == \"provider\":\n            if field_value == \"OpenAI\":\n                build_config[\"model_name\"][\"options\"] = OPENAI_CHAT_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES\n                build_config[\"model_name\"][\"value\"] = OPENAI_CHAT_MODEL_NAMES[0]\n                build_config[\"api_key\"][\"display_name\"] = \"OpenAI API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"Anthropic\":\n                build_config[\"model_name\"][\"options\"] = ANTHROPIC_MODELS\n                build_config[\"model_name\"][\"value\"] = ANTHROPIC_MODELS[0]\n                build_config[\"api_key\"][\"display_name\"] = \"Anthropic API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"Google\":\n                build_config[\"model_name\"][\"options\"] = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"value\"] = GOOGLE_GENERATIVE_AI_MODELS[0]\n                build_config[\"api_key\"][\"display_name\"] = \"Google API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"IBM watsonx.ai\":\n                build_config[\"model_name\"][\"options\"] = IBM_WATSONX_DEFAULT_MODELS\n                build_config[\"model_name\"][\"value\"] = IBM_WATSONX_DEFAULT_MODELS[0]\n                build_config[\"api_key\"][\"display_name\"] = \"IBM API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = True\n                build_config[\"project_id\"][\"show\"] = True\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"Ollama\":\n                # Fetch Ollama models from the API\n                build_config[\"api_key\"][\"show\"] = False\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = True\n\n                # Try multiple sources to get the URL (in order of preference):\n                # 1. Instance attribute (already resolved from global/db)\n                # 2. Build config value (may be a global variable reference)\n                # 3. Default value\n                ollama_url = getattr(self, \"ollama_base_url\", None)\n                if not ollama_url:\n                    config_value = build_config[\"ollama_base_url\"].get(\"value\", DEFAULT_OLLAMA_URL)\n                    # If config_value looks like a variable name (all caps with underscores), use default\n                    is_variable_ref = (\n                        config_value\n                        and isinstance(config_value, str)\n                        and config_value.isupper()\n                        and \"_\" in config_value\n                    )\n                    if is_variable_ref:\n                        await logger.adebug(\n                            f\"Config value appears to be a variable reference: {config_value}, using default\"\n                        )\n                        ollama_url = DEFAULT_OLLAMA_URL\n                    else:\n                        ollama_url = config_value\n\n                await logger.adebug(f\"Fetching Ollama models for provider switch. URL: {ollama_url}\")\n                if await is_valid_ollama_url(url=ollama_url):\n                    try:\n                        models = await get_ollama_models(\n                            base_url_value=ollama_url,\n                            desired_capability=DESIRED_CAPABILITY,\n                            json_models_key=JSON_MODELS_KEY,\n                            json_name_key=JSON_NAME_KEY,\n                            json_capabilities_key=JSON_CAPABILITIES_KEY,\n                        )\n                        build_config[\"model_name\"][\"options\"] = models\n                        build_config[\"model_name\"][\"value\"] = models[0] if models else \"\"\n                    except ValueError:\n                        await logger.awarning(\"Failed to fetch Ollama models. Setting empty options.\")\n                        build_config[\"model_name\"][\"options\"] = []\n                        build_config[\"model_name\"][\"value\"] = \"\"\n                else:\n                    await logger.awarning(f\"Invalid Ollama URL: {ollama_url}\")\n                    build_config[\"model_name\"][\"options\"] = []\n                    build_config[\"model_name\"][\"value\"] = \"\"\n        elif (\n            field_name == \"base_url_ibm_watsonx\"\n            and field_value\n            and hasattr(self, \"provider\")\n            and self.provider == \"IBM watsonx.ai\"\n        ):\n            # Fetch IBM models when base_url changes\n            try:\n                models = self.fetch_ibm_models(base_url=field_value)\n                build_config[\"model_name\"][\"options\"] = models\n                build_config[\"model_name\"][\"value\"] = models[0] if models else IBM_WATSONX_DEFAULT_MODELS[0]\n                info_message = f\"Updated model options: {len(models)} models found in {field_value}\"\n                logger.info(info_message)\n            except Exception:  # noqa: BLE001\n                logger.exception(\"Error updating IBM model options.\")\n        elif field_name == \"ollama_base_url\":\n            # Fetch Ollama models when ollama_base_url changes\n            # Use the field_value directly since this is triggered when the field changes\n            logger.debug(\n                f\"Fetching Ollama models from updated URL: {build_config['ollama_base_url']} \\\n                and value {self.ollama_base_url}\",\n            )\n            await logger.adebug(f\"Fetching Ollama models from updated URL: {self.ollama_base_url}\")\n            if await is_valid_ollama_url(url=self.ollama_base_url):\n                try:\n                    models = await get_ollama_models(\n                        base_url_value=self.ollama_base_url,\n                        desired_capability=DESIRED_CAPABILITY,\n                        json_models_key=JSON_MODELS_KEY,\n                        json_name_key=JSON_NAME_KEY,\n                        json_capabilities_key=JSON_CAPABILITIES_KEY,\n                    )\n                    build_config[\"model_name\"][\"options\"] = models\n                    build_config[\"model_name\"][\"value\"] = models[0] if models else \"\"\n                    info_message = f\"Updated model options: {len(models)} models found in {self.ollama_base_url}\"\n                    await logger.ainfo(info_message)\n                except ValueError:\n                    await logger.awarning(\"Error updating Ollama model options.\")\n                    build_config[\"model_name\"][\"options\"] = []\n                    build_config[\"model_name\"][\"value\"] = \"\"\n            else:\n                await logger.awarning(f\"Invalid Ollama URL: {self.ollama_base_url}\")\n                build_config[\"model_name\"][\"options\"] = []\n                build_config[\"model_name\"][\"value\"] = \"\"\n        elif field_name == \"model_name\":\n            # Refresh Ollama models when model_name field is accessed\n            if hasattr(self, \"provider\") and self.provider == \"Ollama\":\n                ollama_url = getattr(self, \"ollama_base_url\", DEFAULT_OLLAMA_URL)\n                if await is_valid_ollama_url(url=ollama_url):\n                    try:\n                        models = await get_ollama_models(\n                            base_url_value=ollama_url,\n                            desired_capability=DESIRED_CAPABILITY,\n                            json_models_key=JSON_MODELS_KEY,\n                            json_name_key=JSON_NAME_KEY,\n                            json_capabilities_key=JSON_CAPABILITIES_KEY,\n                        )\n                        build_config[\"model_name\"][\"options\"] = models\n                    except ValueError:\n                        await logger.awarning(\"Failed to refresh Ollama models.\")\n                        build_config[\"model_name\"][\"options\"] = []\n                else:\n                    build_config[\"model_name\"][\"options\"] = []\n\n            # Hide system_message for o1 models - currently unsupported\n            if field_value and field_value.startswith(\"o1\") and hasattr(self, \"provider\") and self.provider == \"OpenAI\":\n                if \"system_message\" in build_config:\n                    build_config[\"system_message\"][\"show\"] = False\n            elif \"system_message\" in build_config:\n                build_config[\"system_message\"][\"show\"] = True\n        return build_config\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input text to send to the model",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "external_options": {},
                "info": "Select the model to use",
                "name": "model_name",
                "options": [
                  "gpt-4o-mini",
                  "gpt-4o",
                  "gpt-4.1",
                  "gpt-4.1-mini",
                  "gpt-4.1-nano",
                  "gpt-4-turbo",
                  "gpt-4-turbo-preview",
                  "gpt-4",
                  "gpt-3.5-turbo",
                  "gpt-5.1",
                  "gpt-5",
                  "gpt-5-mini",
                  "gpt-5-nano",
                  "gpt-5-chat-latest",
                  "o1",
                  "o3-mini",
                  "o3",
                  "o3-pro",
                  "o4-mini",
                  "o4-mini-high"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "gpt-4o-mini"
              },
              "ollama_base_url": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Ollama API URL",
                "dynamic": false,
                "info": "Endpoint of the Ollama API (Ollama only). Defaults to http://localhost:11434",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "ollama_base_url",
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "project_id": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "watsonx Project ID",
                "dynamic": false,
                "info": "The project ID associated with the foundation model (IBM watsonx.ai only)",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "project_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "provider": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {},
                "info": "Select the model provider",
                "name": "provider",
                "options": [
                  "OpenAI",
                  "Anthropic",
                  "Google",
                  "IBM watsonx.ai",
                  "Ollama"
                ],
                "options_metadata": [
                  {
                    "icon": "OpenAI"
                  },
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "WatsonxAI"
                  },
                  {
                    "icon": "Ollama"
                  }
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "OpenAI"
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Whether to stream the response",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "A system message that helps set the behavior of the assistant",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "SliderInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "Controls randomness in responses",
                "max_label": "",
                "max_label_icon": "",
                "min_label": "",
                "min_label_icon": "",
                "name": "temperature",
                "override_skip": false,
                "placeholder": "",
                "range_spec": {
                  "max": 1,
                  "min": 0,
                  "step": 0.01,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "title_case": false,
                "tool_mode": false,
                "track_in_telemetry": false,
                "type": "slider",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "LanguageModelComponent"
        },
        "dragging": false,
        "id": "LanguageModelComponent-JQa1y",
        "measured": {
          "height": 530,
          "width": 320
        },
        "position": {
          "x": 798.3373209745413,
          "y": 659.0677011104727
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": 127.02773135695224,
      "y": 112.53060944894526,
      "zoom": 0.5933975466927722
    }
  },
  "description": "Support Hybrid Search",
  "endpoint_name": null,
  "id": "44d25ca6-d599-491a-81b2-354659679c30",
  "is_component": false,
  "last_tested_version": "1.7.3",
  "name": "Support Hybrid Search v2 (3)",
  "tags": [
    "openai",
    "astradb",
    "rag",
    "q-a"
  ]
}